{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ceb9349",
   "metadata": {},
   "source": [
    "## Project1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be38e72",
   "metadata": {},
   "source": [
    "### 1. Chosing of  label\n",
    "First I need to specify the target this project want to predict. Home price index is a monthly renewed index but with data revisioned. The index itself is non-stationary, so the label should be the next month price return. However, the data can be updated during different periods, so my choice of the label is as follows:\n",
    "With every update of the price index, I will do a forecast for its one month future return. Hence the label used for training should be recalculated if the index itself is revisioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a24e45cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fredapi import Fred\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "import datetime \n",
    "import keras\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "fred = Fred(api_key='71b1e448bbe0374c7eb29ddd9907b7aa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3aabfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fred.get_series_all_releases('CSUSHPINSA')\n",
    "label = data.set_index(['realtime_start', 'date'])['value'].unstack().fillna(method = 'ffill')\n",
    "label = pd.DataFrame(label.values[:, 1:]/label.values[:, :-1] - 1, index = label.index,  columns = label.columns[:-1])\n",
    "check = label.stack().reset_index().groupby(['realtime_start'])['date'].last().reset_index()\n",
    "assert (check['realtime_start'] < check['date']).sum() == 0\n",
    "label = label.stack().groupby(level = ['realtime_start']).last()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abcf227",
   "metadata": {},
   "source": [
    "### 2. Chosing of  features\n",
    "\n",
    "From Fred DataBase I choosed several features as follows:\n",
    "\n",
    "1. unemployment rate: demand  of people want to invest.\n",
    "2. gdp: the nation's general wealth\n",
    "3. long-short interest rate/30Year Mortgage Rate: general investing leverage. \n",
    "4. working-age population/total population: demand of investing the house. \n",
    "5. m2: overall liquidity of the currency\n",
    "6. cpi/inflation: cost of general commodity products\n",
    "7. median housing sales: expected willingness of investing in house\n",
    "8. sp500：relatively high risk assets pricing level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d3179ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [01:15<00:00,  6.88s/it]\n"
     ]
    }
   ],
   "source": [
    "sym_dict = {\n",
    "    \"umemploy\": \"UNRATE\",\n",
    "    \"gdp\": 'GDP',\n",
    "    \"lsinterest\":\"T10Y2Y\",\n",
    "    \"linterest\":\"MORTGAGE30US\",\n",
    "    \"m2\":\"M2SL\",\n",
    "    \"cpi\": \"CPIAUCSL\",\n",
    "    \"median_sales\": \"MSPUS\",\n",
    "    \"inflation\": \"T10YIE\",\n",
    "    \"working_pop\": \"LFWA64TTUSM647S\",\n",
    "    \"totpop\": \"POPTHM\",\n",
    "    \"sp500\": \"SP500\"    \n",
    "            }\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in tqdm(sym_dict.keys()):\n",
    "    if i!=  \"lsinterest\" and i != \"sp500\" and i != \"inflation\":\n",
    "        res = fred.get_series_all_releases(sym_dict[i])\n",
    "    else:\n",
    "        res = fred.get_series(sym_dict[i])\n",
    "    data_dict[i] =  res \n",
    "    \n",
    "    \n",
    "\n",
    "type_dict = {\n",
    "    \"D\": [\"sp500\", \"lsinterest\", \"inflation\"],\n",
    "    \"W\": [\"linterest\"],\n",
    "    \"M\": ['umemploy', \"m2\", 'cpi', \"working_pop\", \"totpop\"],\n",
    "    \"Q\": [\"gdp\", \"median_sales\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f8fe86",
   "metadata": {},
   "source": [
    "###  3. Preprocessing of feataures\n",
    "\n",
    "1. Daily frequency data: SP500, longshort interest, inflation\n",
    "\n",
    "    For every label, I choose previous 252 days data observations for each feature as the input of model. Especially for Sp500, I normalize the data by divide the last day price. Hence for each prediction, a matrix of dimension (252, 3) is inputed as the daily feature.\n",
    "\n",
    "2. Other frequency data:\n",
    "    \n",
    "    For weekly, monthly, quarterly data, there are data revision at different time periods. The preprocess algo runs following logic:\n",
    "    1. unpivot the data by the issued date(realtime_start) and report period(date)\n",
    "    2. forfilling the nans\n",
    "    3. for each timeindex in label, find the latest available X period data (X = 52, 24, 8 for weekly, monthly, quarterly data)\n",
    "    4. for \"gdp\", \"m2\", 'cpi', 'median_sales', 'working_pop', 'tot_pop', those features are normalized by divide its last value.\n",
    "    \n",
    "    \n",
    "    Following the same logic for daily frequency data, for each prediction, weekly/monthly/quarterly data is inputed as a matrix of (52, 1), (24, 5), (8,2) \n",
    "   \n",
    "Additionaly, if the data inputed  has nans, it will be filled by mean of the corresponding feature. \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43473dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDailyFeatures(label, data_dict, name, D_lag):\n",
    "    lst = []\n",
    "    for i in label.index:\n",
    "        arr = data_dict[name][:i].iloc[:-1].tail(D_lag).values\n",
    "        arr[np.isnan(arr)] = np.nanmean(arr)\n",
    "        if name == \"sp500\":\n",
    "            last = arr[~np.isnan(arr)][-1]\n",
    "            arr = arr/last\n",
    "        lst.append(np.expand_dims(arr, axis = 1))\n",
    "    \n",
    "    lst = np.concatenate(lst, axis = 1).T\n",
    "    lst = pd.DataFrame(lst, index = label.index)    \n",
    "    return lst \n",
    "\n",
    "\n",
    "def processOtherFeatures(label, data_dict, name, lag):\n",
    "    \n",
    "    lst = []\n",
    "    for i in label.index:\n",
    "        dateprev = pd.to_datetime(i) - datetime.timedelta(days = 1)\n",
    "        tmp = data_dict[name].set_index(['realtime_start', 'date'])['value'].unstack().fillna(method = 'ffill')\n",
    "        arr = tmp[:dateprev].iloc[-1,:].dropna().values[-lag:]\n",
    "        arr[np.isnan(arr)] = np.nanmean(arr)\n",
    "        if name in [\"gdp\", \"m2\", 'cpi', 'median_sales', 'working_pop', 'tot_pop']:\n",
    "            last = arr[~np.isnan(arr)][-1]\n",
    "            arr = arr/last\n",
    "        lst.append(np.expand_dims(arr, axis = 1))\n",
    "    \n",
    "    lst = np.concatenate(lst, axis = 1).T\n",
    "    lst = pd.DataFrame(lst, index = label.index)    \n",
    "    return lst \n",
    "\n",
    "def processFeatureArr(label, data_dict, type_dict):\n",
    "    lst = []\n",
    "    for name in type_dict.keys():\n",
    "        res = []\n",
    "        if name == \"D\":\n",
    "            for j in tqdm(type_dict[name], desc = \"processing\" + name):\n",
    "                tmp =  processDailyFeatures(label, data_dict, j, 252)\n",
    "                tmp = np.expand_dims(tmp.values, axis = 2)\n",
    "                res.append(tmp)\n",
    "        \n",
    "        else:\n",
    "            lag_dict  = {'W':52,\n",
    "                        'M':24,\n",
    "                         'Q':8\n",
    "                        }\n",
    "            for j in tqdm(type_dict[name], desc = \"processing\" + name):\n",
    "                tmp =  processOtherFeatures(label, data_dict, j, lag_dict[name])\n",
    "                tmp = np.expand_dims(tmp.values, axis = 2)\n",
    "                res.append(tmp)\n",
    "        \n",
    "        res = np.concatenate(res, axis = -1)\n",
    "        lst.append(res)    \n",
    "    return  lst "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd63c236",
   "metadata": {},
   "source": [
    "###  4. Building Model\n",
    "\n",
    "neural nets is chosen as the model, since nn has the capability for merging features with data of different frequency. For the sturcture of the model, four LSTM is used for merging four frequency (daily, weekly, monthly, quarterly) data. the four embeddings pass through a concat layer and a dense network is applied to fit the label. Further mse is used as the loss function and total data is splitted in train and val, which contains 90 samples and 17 samples respectively. A learning rate scheduler is applied to the fitting process, a decreasing factor would be applied to learning rate if val loss is not decreasing. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b438c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myModel():\n",
    "    inputs = keras.Input(shape = [252, 3], name = \"input_0\")\n",
    "    inputs2 = keras.Input(shape = [52, 1], name = \"input_1\")\n",
    "    inputs3 = keras.Input(shape = [24, 5], name = \"input_2\")\n",
    "    inputs4 = keras.Input(shape = [8, 2], name = \"input_3\")\n",
    "        \n",
    "    x1 = keras.layers.LSTM(8)(inputs)\n",
    "    x2 = keras.layers.LSTM(8)(inputs2)\n",
    "    x3 = keras.layers.LSTM(8)(inputs3)\n",
    "    x4 = keras.layers.LSTM(8)(inputs4)\n",
    "\n",
    "    z = K.concatenate([x1,x2,x3,x4])\n",
    "    outputs = keras.layers.Dense(20,  activation = 'relu')(z)\n",
    "    outputs = keras.layers.Dense(1)(outputs)\n",
    "    outputs = keras.layers.BatchNormalization()(outputs)\n",
    "    model = keras.Model(inputs = [inputs, inputs2, inputs3, inputs4], outputs = outputs)\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cb8eb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processingD: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 45.64it/s]\n",
      "processingW: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:50<00:00, 50.22s/it]\n",
      "processingM: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [01:19<00:00, 15.81s/it]\n",
      "processingQ: 100%|███████████████████████████████████████████████████████████████████████| 2/2 [00:06<00:00,  3.14s/it]\n"
     ]
    }
   ],
   "source": [
    "lst = processFeatureArr(label, data_dict, type_dict)\n",
    "lst.append(label.values)\n",
    "\n",
    "datasetX = tf.data.Dataset.from_tensor_slices((lst[0][:90], \n",
    "                                              lst[1][:90],\n",
    "                                               lst[2][:90],\n",
    "                                               lst[3][:90]\n",
    "                                              ))\n",
    "datasetY = tf.data.Dataset.from_tensor_slices(lst[4][:90])\n",
    "traindata = tf.data.Dataset.zip((datasetX, datasetY)).batch(10)\n",
    "\n",
    "datasetX = tf.data.Dataset.from_tensor_slices((lst[0][90:], \n",
    "                                              lst[1][90:],\n",
    "                                               lst[2][90:],\n",
    "                                               lst[3][90:]\n",
    "                                              ))\n",
    "datasetY = tf.data.Dataset.from_tensor_slices(lst[4][90:])\n",
    "valdata = tf.data.Dataset.zip((datasetX, datasetY)).batch(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1583e15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "9/9 [==============================] - 7s 172ms/step - loss: 0.2089 - val_loss: 0.1083 - lr: 5.0000e-04\n",
      "Epoch 2/200\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.0934 - val_loss: 0.0513 - lr: 5.0000e-04\n",
      "Epoch 3/200\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0401 - val_loss: 0.0201 - lr: 5.0000e-04\n",
      "Epoch 4/200\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0200 - val_loss: 0.0085 - lr: 5.0000e-04\n",
      "Epoch 5/200\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0162 - val_loss: 0.0046 - lr: 5.0000e-04\n",
      "Epoch 6/200\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.0136 - val_loss: 0.0039 - lr: 5.0000e-04\n",
      "Epoch 7/200\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0106 - val_loss: 0.0050 - lr: 5.0000e-04\n",
      "Epoch 8/200\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0090 - val_loss: 0.0059 - lr: 5.0000e-04\n",
      "Epoch 9/200\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.0079 - val_loss: 0.0053 - lr: 5.0000e-04\n",
      "Epoch 10/200\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.0073 - val_loss: 0.0048 - lr: 3.7500e-04\n",
      "Epoch 11/200\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0069 - val_loss: 0.0040 - lr: 3.7500e-04\n",
      "Epoch 12/200\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0066 - val_loss: 0.0033 - lr: 3.7500e-04\n",
      "Epoch 13/200\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0063 - val_loss: 0.0030 - lr: 3.7500e-04\n",
      "Epoch 14/200\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.0061 - val_loss: 0.0024 - lr: 3.7500e-04\n",
      "Epoch 15/200\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0059 - val_loss: 0.0022 - lr: 3.7500e-04\n",
      "Epoch 16/200\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0057 - val_loss: 0.0018 - lr: 3.7500e-04\n",
      "Epoch 17/200\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.0056 - val_loss: 0.0016 - lr: 3.7500e-04\n",
      "Epoch 18/200\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.0054 - val_loss: 0.0013 - lr: 3.7500e-04\n",
      "Epoch 19/200\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.0052 - val_loss: 0.0011 - lr: 3.7500e-04\n",
      "Epoch 20/200\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0050 - val_loss: 0.0011 - lr: 3.7500e-04\n",
      "Epoch 21/200\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0048 - val_loss: 0.0010 - lr: 3.7500e-04\n",
      "Epoch 22/200\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0047 - val_loss: 0.0011 - lr: 3.7500e-04\n",
      "Epoch 23/200\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0046 - val_loss: 0.0012 - lr: 3.7500e-04\n",
      "Epoch 24/200\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0045 - val_loss: 0.0014 - lr: 3.7500e-04\n",
      "Epoch 25/200\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.0043 - val_loss: 0.0015 - lr: 2.8125e-04\n",
      "Epoch 26/200\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0042 - val_loss: 0.0017 - lr: 2.8125e-04\n",
      "Epoch 27/200\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.0041 - val_loss: 0.0019 - lr: 2.8125e-04\n",
      "Epoch 28/200\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.0040 - val_loss: 0.0020 - lr: 2.1094e-04\n",
      "Epoch 29/200\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0040 - val_loss: 0.0022 - lr: 2.1094e-04\n",
      "Epoch 30/200\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0039 - val_loss: 0.0025 - lr: 2.1094e-04\n",
      "Epoch 31/200\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.0038 - val_loss: 0.0027 - lr: 1.5820e-04\n",
      "Epoch 32/200\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0038 - val_loss: 0.0028 - lr: 1.5820e-04\n",
      "Epoch 33/200\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0037 - val_loss: 0.0029 - lr: 1.5820e-04\n",
      "Epoch 34/200\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0037 - val_loss: 0.0030 - lr: 1.1865e-04\n",
      "Epoch 35/200\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0036 - val_loss: 0.0032 - lr: 1.1865e-04\n",
      "Epoch 36/200\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0036 - val_loss: 0.0033 - lr: 1.1865e-04\n",
      "Epoch 37/200\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0036 - val_loss: 0.0035 - lr: 8.8989e-05\n",
      "Epoch 38/200\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 0.0037 - lr: 8.8989e-05\n",
      "Epoch 39/200\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0035 - val_loss: 0.0039 - lr: 8.8989e-05\n",
      "Epoch 40/200\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 0.0042 - lr: 6.6742e-05\n",
      "Epoch 41/200\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0035 - val_loss: 0.0046 - lr: 6.6742e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f770b58750>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TerminateOnNaN, ReduceLROnPlateau\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "model = myModel()\n",
    "\n",
    "name = ''.join(str(datetime.datetime.now())[11:].split('.')[0].split(':'))\n",
    "model_name = 'MYMODEL'+ name\n",
    "\n",
    "nan_stop = TerminateOnNaN()\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(model_name))\n",
    "stop = EarlyStopping(monitor=\"val_loss\", patience = 20, restore_best_weights = True)\n",
    "learning_rate_decay = ReduceLROnPlateau(monitor=\"val_loss\", \n",
    "                                        factor=0.75, patience = 3, min_delta= 1e-5)\n",
    "\n",
    "model.compile(loss = \"mse\",\n",
    "    optimizer = Adam(5e-4, )   )\n",
    "model.fit(traindata.cache(),\n",
    "          validation_data=valdata.cache(),\n",
    "          callbacks = [stop, tensorboard, learning_rate_decay, nan_stop], \n",
    "          shuffle = False,\n",
    "          epochs= 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1100c1f",
   "metadata": {},
   "source": [
    "### 5. Comment  on  the results\n",
    "\n",
    "Although the val loss is decreasing when epoch increases, the mse is still not good since the best rmse loss is around 1%. After checking the valdata, it can be founded that the result in 2023 Q3 has not been predicted very well by the model. Hence, the model may be imporved by several points as follow:\n",
    "\n",
    "1. More features: some alternative data can be applied to forecast the housing price, such as sales data crawled from different housing website or renting agency. Sentimental analysis using NLP techniques can also be used as the sentimental scores for predicting the housing price. The problem of using economy data is the frequency of the data is low and sometimes signal is lagged, hence using higher frequency alternative data may help predicting the price index.\n",
    "\n",
    "2. Different architecture of the model: timestep for different data can be varied, however optimization the structure can be very time-consuming. And tree model can be used as the model alternatively for caputring the cross information of different features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f191365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 21ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real</th>\n",
       "      <th>sig</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realtime_start</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-05-31</th>\n",
       "      <td>0.025536</td>\n",
       "      <td>-0.001061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-28</th>\n",
       "      <td>0.020779</td>\n",
       "      <td>0.014034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>0.014937</td>\n",
       "      <td>0.020205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-30</th>\n",
       "      <td>0.005901</td>\n",
       "      <td>0.041288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-27</th>\n",
       "      <td>-0.003310</td>\n",
       "      <td>0.052341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-25</th>\n",
       "      <td>-0.010686</td>\n",
       "      <td>-0.003190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-29</th>\n",
       "      <td>-0.010083</td>\n",
       "      <td>-0.025004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-27</th>\n",
       "      <td>-0.005316</td>\n",
       "      <td>-0.032403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-31</th>\n",
       "      <td>-0.005579</td>\n",
       "      <td>0.004940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-28</th>\n",
       "      <td>-0.008122</td>\n",
       "      <td>0.030658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-28</th>\n",
       "      <td>-0.005484</td>\n",
       "      <td>0.010825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-25</th>\n",
       "      <td>0.001910</td>\n",
       "      <td>0.023228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-30</th>\n",
       "      <td>0.012567</td>\n",
       "      <td>0.010656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-27</th>\n",
       "      <td>0.013087</td>\n",
       "      <td>0.010535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-25</th>\n",
       "      <td>0.012160</td>\n",
       "      <td>-0.010472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-29</th>\n",
       "      <td>0.009339</td>\n",
       "      <td>-0.036863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-26</th>\n",
       "      <td>0.005974</td>\n",
       "      <td>-0.073349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    real       sig\n",
       "realtime_start                    \n",
       "2022-05-31      0.025536 -0.001061\n",
       "2022-06-28      0.020779  0.014034\n",
       "2022-07-26      0.014937  0.020205\n",
       "2022-08-30      0.005901  0.041288\n",
       "2022-09-27     -0.003310  0.052341\n",
       "2022-10-25     -0.010686 -0.003190\n",
       "2022-11-29     -0.010083 -0.025004\n",
       "2022-12-27     -0.005316 -0.032403\n",
       "2023-01-31     -0.005579  0.004940\n",
       "2023-02-28     -0.008122  0.030658\n",
       "2023-03-28     -0.005484  0.010825\n",
       "2023-04-25      0.001910  0.023228\n",
       "2023-05-30      0.012567  0.010656\n",
       "2023-06-27      0.013087  0.010535\n",
       "2023-07-25      0.012160 -0.010472\n",
       "2023-08-29      0.009339 -0.036863\n",
       "2023-09-26      0.005974 -0.073349"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot = pd.concat([label[90:], pd.DataFrame(model.predict(valdata), index = label.index[90:])], axis = 1)\n",
    "tot.columns  = ['real', 'sig']\n",
    "tot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e0a70",
   "metadata": {},
   "source": [
    "## Project 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7a65cb",
   "metadata": {},
   "source": [
    "In this section I will show a signal I am currently following and I thought the logic behind it was reasonable.\n",
    "\n",
    "To have a brief background introduction, in Chinese Ashare Market, CSI300 Index contains the core assets of Chinese Company, just like the SP500. It is also a large cap index since it contains 300 largest market cap stocks in China. On the contrary, CSI1000 is a small cap index which contains bottom  1000 market cap from top 1800 stocks in Ashare market.\n",
    "\n",
    "From Year 2023, the relationship between China and US is getting a lot worse and the whole world enconmy is getting tightened after the US raising the interest rate. Anyone investing in Chinese Market will think about the Fx risk ---- was it worth enough for risking holding a weak currency or not. A reasonable idea would be leaving the chinese market when US dollar outperfoms RMB, and vice versa.    \n",
    "\n",
    "Hence, I guess when USD-CNH is going up, foreign invester would leave Chinese Market and causing a decrease in the coreasset Index CSI-300. Further, the selling power should not lasting for long duration, the market will repair after they leave the market, hence only a short time period of price dynamics is predictable. Last, A small-cap index can work as a benchmark to hedge the overall market systematic risk. We can just use the spread between CSI-300 and CSI-1000 to achieve this. \n",
    "\n",
    "CSI300 and CSI1000 both have corresponding futures, but in the following code I just assumed the index itself can be long or short for simiplicity. And since I just want to show the trading idea, certain parameters are chosen subjectively and not be tunned.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0037ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "#get two index(large cap and small cap index) future return\n",
    "#forecast_start indicates the no. of minbar  to count for start, same for forecast_end,\n",
    "#since this is 1-minibar data, forecast_start and forecast end ranges from 0-239\n",
    "def getForecastLabel(forecast_start, forecast_end):    \n",
    "    price = pd.read_csv('./price.csv',index_col = 0)\n",
    "\n",
    "    large_cap = price[price['StockID'] == 'SH000300'].copy()\n",
    "    large_cap.loc[:, 'no_bar'] = np.arange(len(large_cap))%240\n",
    "    large_cap.index = pd.to_datetime(large_cap.index)\n",
    "\n",
    "    small_cap = price[price['StockID'] == 'SH000852'].copy()\n",
    "    small_cap.loc[:, 'no_bar'] = np.arange(len(small_cap))%240\n",
    "    small_cap.index = pd.to_datetime(small_cap.index)\n",
    "\n",
    "    \n",
    "    price_in = large_cap[large_cap['no_bar'] == forecast_start]['open']\n",
    "    price_out = large_cap[large_cap['no_bar'] == forecast_end]['close']\n",
    "    price_in.index = price_in.index.date\n",
    "    price_out.index = price_out.index.date\n",
    "    label = price_out/price_in - 1\n",
    "    label.index = pd.to_datetime(label.index)\n",
    "\n",
    "    price_in = small_cap[small_cap['no_bar'] == forecast_start]['open']\n",
    "    price_out = small_cap[small_cap['no_bar'] == forecast_end]['close']\n",
    "    price_in.index = price_in.index.date\n",
    "    price_out.index = price_out.index.date\n",
    "    label2 = price_out/price_in - 1\n",
    "    label2.index = pd.to_datetime(label2.index)\n",
    "    label = pd.concat([label, label2], axis = 1)\n",
    "    label.columns  = ['label_largecap', 'label_smallcap']\n",
    "    return label\n",
    "\n",
    "#get the signal -- which  is the return of the fx rate\n",
    "#timeperiod is the range of the fx periods used, isPrev inidicates whether  the time  is  previous date or not \n",
    "def getSigPeriod(fx, timeperiod, isPrev):\n",
    "    sig_start, sig_end = timeperiod\n",
    "    isPrev_start, isPrev_end = isPrev\n",
    "    \n",
    "    time_search = pd.DataFrame(label.index, index = pd.to_datetime(label.index), columns = ['prev_date']).shift(1).dropna()\n",
    "    \n",
    "    if isPrev_start:\n",
    "        time_search['sig_start'] =  pd.to_datetime([str(i)[:10] + \" \" + sig_start for i in time_search['prev_date']])\n",
    "    else:\n",
    "        time_search['sig_start'] =  pd.to_datetime([str(i)[:10] + \" \" + sig_start for i in time_search.index])\n",
    "    \n",
    "    if isPrev_end:\n",
    "        time_search['sig_end'] = pd.to_datetime([str(i)[:10] + \" \" + sig_end for i in time_search['prev_date']])\n",
    "    else:\n",
    "        time_search['sig_end'] = pd.to_datetime([str(i)[:10] + \" \" + sig_end for i in time_search.index])\n",
    "    \n",
    "    \n",
    "    \n",
    "    sigp = [fx.loc[i:, 'Open'].values[0] for i in time_search['sig_start']]\n",
    "    sign = [fx.loc[i:, 'Open'].values[0] for i in time_search['sig_end']]\n",
    "    sig = np.array(sign)/np.array(sigp) - 1\n",
    "    sig = pd.Series(sig, index = pd.to_datetime([str(i)[:10] for i in time_search['sig_end']]))\n",
    "    res = pd.concat([sig, label], axis = 1).dropna()\n",
    "    res.columns= ['sig', 'label_largecap', 'label_smallcap']\n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b046746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USDCNH=X: No price data found, symbol may be delisted (1h 2021-12-31 -> 2023-09-28)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 9\u001b[0m\n\u001b[0;32m      3\u001b[0m ticker \u001b[38;5;241m=\u001b[39m yf\u001b[38;5;241m.\u001b[39mTicker(symbol)\n\u001b[0;32m      4\u001b[0m fx \u001b[38;5;241m=\u001b[39m ticker\u001b[38;5;241m.\u001b[39mhistory(start \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2021-12-31\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m                        end \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023-09-28\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m                       interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1h\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m                       actions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      8\u001b[0m                       auto_adjust\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 9\u001b[0m fx\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(fx\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mtz_convert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAsia/Shanghai\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m fx\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(fx\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5895\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5896\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   5897\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   5898\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   5899\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5900\u001b[0m ):\n\u001b[0;32m   5901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 5902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\accessor.py:182\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor\n\u001b[1;32m--> 182\u001b[0m accessor_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor(obj)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\accessors.py:512\u001b[0m, in \u001b[0;36mCombinedDatetimelikeProperties.__new__\u001b[1;34m(cls, data)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_period_dtype(data\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PeriodProperties(data, orig)\n\u001b[1;32m--> 512\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only use .dt accessor with datetimelike values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "#fetch fx rates\n",
    "symbol = \"usdcnh=x\"\n",
    "ticker = yf.Ticker(symbol)\n",
    "fx = ticker.history(start = '2021-12-31',\n",
    "                       end = '2023-09-28',\n",
    "                      interval='1h',\n",
    "                      actions=True,\n",
    "                      auto_adjust=True)\n",
    "fx.index = pd.Series(fx.index).dt.tz_convert('Asia/Shanghai')\n",
    "fx.index = pd.Series(fx.index).dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef75dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hold the spread for 10minutes, and closed the position intraday\n",
    "#the fx period I looked is 4am to 9am in CST, since this is the time when US market shutdown and the Chinsese Market Not Opened.\n",
    "\n",
    "\n",
    "forecast = 0, 10\n",
    "label = getForecastLabel(*forecast)\n",
    "tot = getSigPeriod(fx, (\"04:00:00\", '09:00:00'), (False, False))\n",
    "tot['2023-01-01':].corr()\n",
    "\n",
    "#We can see especially from 2023-01-01, the correlation between fxrates and two indices diverge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5729628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using zscore, the threshold is set to be 0.5\n",
    "\n",
    "window  = 60\n",
    "zscore = (tot['sig'] -  tot['sig'].rolling(window = window).mean())/tot['sig'].rolling(window = window).std()\n",
    "spread = tot['label_largecap'] - tot['label_smallcap']\n",
    "strats =  pd.concat([zscore, spread], axis = 1).dropna()\n",
    "strats.columns =  ['zscore',  'spread']\n",
    "strats['pos'] = 0\n",
    "strats.loc[strats['zscore']<= -0.5, 'pos' ]  = 1\n",
    "strats.loc[strats['zscore']>= 0.5, 'pos'] = -1\n",
    "strats[['spread', 'pos']].prod(axis = 1).cumsum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c6807b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afaaafc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
